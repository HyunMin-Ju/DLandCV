{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms,models,datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import cv2, glob, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d iiplutocrat45ii/painting-vs-photograph-classification-dataset\n",
    "!unzip painting-vs-photograph-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'train/'\n",
    "valid_data_dir = 'valid/'\n",
    "test_data_dir = 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotosPainting(Dataset):\n",
    "    def __init__(self, folder, num_images = 1500):\n",
    "        photos = glob(folder + 'photos/*.jpg') # photo images 경로 설정\n",
    "        painting = glob(folder + 'painting/*.jpg') # painting images 경로 설정\n",
    "        self.fpaths = photos[:num_images] + painting[:num_images]\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        from random import shuffle, seed; seed(10); shuffle(self.fpaths)\n",
    "        self.targets = [True if 'photos' in path_name else False for path_name in self.fpaths]\n",
    "    def __len__(self): return len(self.fpaths)\n",
    "    def __getitem__(self, ix):\n",
    "        f = self.fpaths[ix]\n",
    "        target = self.targets[ix]\n",
    "        im = (cv2.imread(f)[:,:,::-1])\n",
    "        im = cv2.resize(im, (224,224))\n",
    "        im = torch.tensor(im/255)\n",
    "        im = im.permute(2,0,1)\n",
    "        im = self.normalize(im) \n",
    "        return im.float().to(device), torch.tensor([target]).float().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PhotosPainting(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, label = data[200]\n",
    "plt.imshow(im.permute(1,2,0).cpu())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    for param in model.features[:24].parameters():\n",
    "        param.require_grad = False\n",
    "\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "    model.classifier = nn.Sequential(nn.Flatten(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid())\n",
    "    loss_fn = nn.BCELoss()\n",
    "      \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3)\n",
    "    return model.to(device), loss_fn, optimizer\n",
    "model, loss_fn, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model, criterion, optimizer = get_model()\n",
    "summary(model, torch.zeros(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, model, opt, loss_fn):\n",
    "    model.train()\n",
    "    prediction = model(x)\n",
    "    batch_loss = loss_fn(prediction, y)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return batch_loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(x, y, model):\n",
    "    model.eval()\n",
    "    prediction = model(x)\n",
    "    is_correct = (prediction > 0.5) == y\n",
    "    return is_correct.cpu().numpy().tolist()\n",
    "  \n",
    "def get_data():\n",
    "    train = PhotosPainting(train_data_dir, num_images = 3000)\n",
    "    trn_dl = DataLoader(train, batch_size=32, shuffle=True, drop_last = True)\n",
    "    val = PhotosPainting(valid_data_dir, num_images = 1500)\n",
    "    val_dl = DataLoader(val, batch_size=32, shuffle=True, drop_last = True)\n",
    "    return trn_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl, val_dl = get_data()\n",
    "model, loss_fn, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies = [], []\n",
    "val_accuracies = []\n",
    "num_epoch = 20\n",
    "for epoch in range(num_epoch):\n",
    "    #print(f\" epoch {epoch + 1}/{num_epoch}\")\n",
    "    train_epoch_losses, train_epoch_accuracies = [], []\n",
    "    val_epoch_accuracies = []\n",
    "\n",
    "    for ix, batch in enumerate(iter(trn_dl)):\n",
    "        x, y = batch\n",
    "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
    "        train_epoch_losses.append(batch_loss) \n",
    "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
    "\n",
    "    for ix, batch in enumerate(iter(trn_dl)):\n",
    "        x, y = batch\n",
    "        is_correct = accuracy(x, y, model)\n",
    "        train_epoch_accuracies.extend(is_correct)\n",
    "    train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "\n",
    "    for ix, batch in enumerate(iter(val_dl)):\n",
    "        x, y = batch\n",
    "        val_is_correct = accuracy(x, y, model)\n",
    "        val_epoch_accuracies.extend(val_is_correct)\n",
    "    val_epoch_accuracy = np.mean(val_epoch_accuracies)\n",
    "\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accuracies.append(train_epoch_accuracy)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "    print(f\"epoch {epoch + 1}/{num_epoch}, train_epoch_loss:{train_epoch_loss}, train_epoch_accuracy: {train_epoch_accuracy}, val_epoch_accuracy: {val_epoch_accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(num_epoch)+1\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "%matplotlib inline\n",
    "plt.plot(epochs, train_accuracies, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation accuracy')\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.title('Training and validation accuracy with VGG16 \\nand 1K training data points')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.ylim(0.95,1)\n",
    "plt.ylim(0.7,1)\n",
    "plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]) \n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accuracies)\n",
    "print(val_accuracies)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
